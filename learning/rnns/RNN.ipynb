{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "\n",
    "Ok, so I'm gonna try to make an RNN learn to speak like Hofstadter, trying to Google as little as possible :P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD GEBB\n",
    "f = open(\"GEB.txt\",\"r\")\n",
    "lines = list(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = list(filter(lambda x: x!= \"\\n\",lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Contents \\n',\n",
       " 'Overview viii \\n',\n",
       " 'List of Illustrations xiv \\n',\n",
       " 'Words of Thanks xix \\n',\n",
       " 'Part I: GEB \\n',\n",
       " 'Introduction: A Musico-Logical Offering 3 \\n',\n",
       " 'Three-Part Invention 29 \\n',\n",
       " 'Chapter I: The MU-puzzle 33 \\n',\n",
       " 'Two-Part Invention 43 \\n',\n",
       " 'Chapter II: Meaning and Form in Mathematics 46 \\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'-\"\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [unicodeToAscii(l) for l in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Contents ',\n",
       " 'Overview viii ',\n",
       " 'List of Illustrations xiv ',\n",
       " 'Words of Thanks xix ',\n",
       " 'Part I GEB ',\n",
       " 'Introduction A Musico-Logical Offering  ',\n",
       " 'Three-Part Invention  ',\n",
       " 'Chapter I The MU-puzzle  ',\n",
       " 'Two-Part Invention  ',\n",
       " 'Chapter II Meaning and Form in Mathematics  ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,;'-\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_letters=len(all_letters)\n",
    "all_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot matrix of first to last letters (not including EOS) for input\n",
    "def inputTensor(line):\n",
    "    tensor = torch.zeros(len(line),1, n_letters)\n",
    "    for li in range(len(line)):\n",
    "        letter = line[li]\n",
    "        tensor[li][0][all_letters.find(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "all_letters.find('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Below are the inputs with which we train our model, which are sentences convereted to one-hot vectors*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line = inputTensor(lines[0])\n",
    "\n",
    "data = [inputTensor(l) for l in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([81, 1, 58])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1201].shape\n",
    "# torch.utils.data.DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Below we make the targets for our model, which are also one-hot vector'd sentences, but shifted by 1 character*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LongTensor of second letter to end (EOS) for target\n",
    "def targetTensor(line):\n",
    "    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n",
    "    letter_indexes.append(n_letters - 1) # EOS\n",
    "    return torch.LongTensor(letter_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [targetTensor(l) for l in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "Ok, how do I make an RNN? Hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2i = nn.Linear(input_size + hidden_size, input_size + hidden_size)\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        combined = F.relu(self.i2i(combined))\n",
    "        hidden = F.tanh(self.i2h(combined))\n",
    "        output = self.i2o(combined)\n",
    "        out = self.softmax(output)\n",
    "        logoutput = self.logsoftmax(output)\n",
    "        return logoutput, out, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return Variable(torch.zeros(1, self.hidden_size))\n",
    "\n",
    "n_hidden = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(n_letters, n_hidden, n_letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Function to get a minibatch randomly sampled from data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "n = len(data)\n",
    "def get_minibatch(batch_size):\n",
    "    indices = random.sample(range(n),batch_size)\n",
    "    return [(data[i],targets[i]) for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_minibatch(2)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sentence():\n",
    "    d = get_minibatch(10)[0][0]\n",
    "    h = Variable(torch.zeros(1,n_hidden))\n",
    "    let = Variable(d[0,:,:])\n",
    "#     print(all_letters[torch.max(let,dim=1)[1]])\n",
    "    lets = [let]\n",
    "    for i in range(d.shape[0]):\n",
    "        _,letter_distribution,h = rnn(let,h)\n",
    "        let_index = torch.multinomial(letter_distribution,1).data[0][0]\n",
    "        let = Variable(torch.zeros(1,n_letters))\n",
    "        let[0,let_index]=1\n",
    "        lets.append(let)\n",
    "    return \"\".join([all_letters[torch.max(let,dim=1)[1]] for let in lets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the Mashiom a does man proof as explain in the program has been the Proofic Paired gal'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 58])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#don't mind me, just checking what targets is\n",
    "# lets[0].shape\n",
    "# targets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The `NLLLoss` stands for negative log-likelihood loss, which means (log of) the probability (likelihood) that each of the right {next characters} are obtained from the softmax distribution that the `rnn` outputs at each point in the sequence.\n",
    "\n",
    "We work with the log of the probability for convenience (the numbers are more manageble, which the computer thanks us for, as it has to store something like -20, instead of 0.0000000000000000001 or something; this helps avoiding accuracy errors). For us, it means that instead of multiplying the likelihoods of the characters to get the likelihood of the sentence, we add the log-likelihoods.\n",
    "\n",
    "We are going to use Adam optimizer, instead of the simpler SGD, because it tends to work better (really beacuse [god Karpathy says so](http://karpathy.github.io/2015/05/21/rnn-effectiveness/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optim = torch.optim.Adam(rnn.parameters())\n",
    "# optim = torch.optim.RMSprop(rnn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "80.49661\n",
      "10\n",
      "77.69018\n",
      "20\n",
      "79.78951\n",
      "30\n",
      "74.845024\n",
      "40\n",
      "82.25401\n",
      "50\n",
      "79.24931\n",
      "60\n",
      "79.22946\n",
      "70\n",
      "77.94086\n",
      "80\n",
      "81.32197\n",
      "90\n",
      "79.94307\n",
      "100\n",
      "77.467224\n",
      "110\n",
      "81.90501\n",
      "120\n",
      "78.57027\n",
      "130\n",
      "82.19566\n",
      "140\n",
      "82.321724\n",
      "150\n",
      "78.31985\n",
      "160\n",
      "77.19246\n",
      "170\n",
      "78.538635\n",
      "180\n",
      "78.44568\n",
      "190\n",
      "79.77867\n",
      "200\n",
      "78.18631\n",
      "210\n",
      "79.288826\n",
      "220\n",
      "76.40116\n",
      "230\n",
      "78.14143\n",
      "240\n",
      "75.48411\n",
      "250\n",
      "75.86159\n",
      "260\n",
      "79.45234\n",
      "270\n",
      "74.76865\n",
      "280\n",
      "75.23619\n",
      "290\n",
      "79.43416\n",
      "300\n",
      "77.186615\n",
      "310\n",
      "79.22845\n",
      "320\n",
      "78.879845\n",
      "330\n",
      "77.11562\n",
      "340\n",
      "78.25997\n",
      "350\n",
      "78.07991\n",
      "360\n",
      "78.06127\n",
      "370\n",
      "75.80647\n",
      "380\n",
      "76.499725\n",
      "390\n",
      "80.51418\n",
      "400\n",
      "76.425\n",
      "410\n",
      "77.14709\n",
      "420\n",
      "78.39279\n",
      "430\n",
      "79.62222\n",
      "440\n",
      "76.35435\n",
      "450\n",
      "79.27382\n",
      "460\n",
      "74.0003\n",
      "470\n",
      "79.40046\n",
      "480\n",
      "81.59311\n",
      "490\n",
      "74.797295\n",
      "500\n",
      "79.38137\n",
      "510\n",
      "77.05853\n",
      "520\n",
      "78.85457\n",
      "530\n",
      "76.55849\n",
      "540\n",
      "80.79766\n",
      "550\n",
      "78.41511\n",
      "560\n",
      "75.72512\n",
      "570\n",
      "78.11564\n",
      "580\n",
      "76.194016\n",
      "590\n",
      "76.21995\n",
      "600\n",
      "75.579185\n",
      "610\n",
      "76.376854\n",
      "620\n",
      "76.7463\n",
      "630\n",
      "79.5023\n",
      "640\n",
      "77.78413\n",
      "650\n",
      "81.6432\n",
      "660\n",
      "79.70269\n",
      "670\n",
      "78.53166\n",
      "680\n",
      "78.053474\n",
      "690\n",
      "81.41713\n",
      "700\n",
      "75.95135\n",
      "710\n",
      "77.263695\n",
      "720\n",
      "75.85729\n",
      "730\n",
      "76.71313\n",
      "740\n",
      "78.66514\n",
      "750\n",
      "77.28721\n",
      "760\n",
      "78.252464\n",
      "770\n",
      "75.95508\n",
      "780\n",
      "77.8668\n",
      "790\n",
      "77.87968\n",
      "800\n",
      "78.91107\n",
      "810\n",
      "79.83455\n",
      "820\n",
      "77.34215\n",
      "830\n",
      "77.95605\n",
      "840\n",
      "75.99301\n",
      "850\n",
      "74.92683\n",
      "860\n",
      "77.05742\n",
      "870\n",
      "80.743\n",
      "880\n",
      "77.17079\n",
      "890\n",
      "76.21281\n",
      "900\n",
      "75.795975\n",
      "910\n",
      "76.65961\n",
      "920\n",
      "78.31902\n",
      "930\n",
      "77.74677\n",
      "940\n",
      "77.41735\n",
      "950\n",
      "81.65127\n",
      "960\n",
      "77.12393\n",
      "970\n",
      "79.96198\n",
      "980\n",
      "84.433495\n",
      "990\n",
      "74.40079\n",
      "1000\n",
      "77.118195\n",
      "1010\n",
      "76.55036\n",
      "1020\n",
      "78.00701\n",
      "1030\n",
      "78.758835\n",
      "1040\n",
      "79.995674\n",
      "1050\n",
      "76.67237\n",
      "1060\n",
      "80.46228\n",
      "1070\n",
      "75.88798\n",
      "1080\n",
      "75.23123\n",
      "1090\n",
      "77.76476\n",
      "1100\n",
      "77.76378\n",
      "1110\n",
      "80.05951\n",
      "1120\n",
      "81.1164\n",
      "1130\n",
      "75.518074\n",
      "1140\n",
      "77.499176\n",
      "1150\n",
      "76.15318\n",
      "1160\n",
      "75.7064\n",
      "1170\n",
      "78.5773\n",
      "1180\n",
      "76.95772\n",
      "1190\n",
      "77.28633\n",
      "1200\n",
      "80.38079\n",
      "1210\n",
      "80.69525\n",
      "1220\n",
      "73.30561\n",
      "1230\n",
      "76.016106\n",
      "1240\n",
      "76.68341\n",
      "1250\n",
      "75.76542\n",
      "1260\n",
      "75.57476\n",
      "1270\n",
      "82.51413\n",
      "1280\n",
      "76.57302\n",
      "1290\n",
      "79.563614\n",
      "1300\n",
      "75.73892\n",
      "1310\n",
      "75.36468\n",
      "1320\n",
      "76.09366\n",
      "1330\n",
      "77.99968\n",
      "1340\n",
      "77.223976\n",
      "1350\n",
      "78.93333\n",
      "1360\n",
      "79.07447\n",
      "1370\n",
      "79.16776\n",
      "1380\n",
      "78.14224\n",
      "1390\n",
      "82.36002\n",
      "1400\n",
      "77.233444\n",
      "1410\n",
      "79.554436\n",
      "1420\n",
      "78.35141\n",
      "1430\n",
      "80.45145\n",
      "1440\n",
      "79.374275\n",
      "1450\n",
      "80.76412\n",
      "1460\n",
      "78.70072\n",
      "1470\n",
      "78.85557\n",
      "1480\n",
      "80.028305\n",
      "1490\n",
      "80.01601\n",
      "1500\n",
      "76.84486\n",
      "1510\n",
      "74.86345\n",
      "1520\n",
      "76.807686\n",
      "1530\n",
      "74.21075\n",
      "1540\n",
      "76.892494\n",
      "1550\n",
      "76.86548\n",
      "1560\n",
      "79.25808\n",
      "1570\n",
      "79.16721\n",
      "1580\n",
      "75.334274\n",
      "1590\n",
      "80.097946\n",
      "1600\n",
      "78.58446\n",
      "1610\n",
      "78.90299\n",
      "1620\n",
      "79.68273\n",
      "1630\n",
      "77.95053\n",
      "1640\n",
      "81.70387\n",
      "1650\n",
      "79.28218\n",
      "1660\n",
      "76.08331\n",
      "1670\n",
      "74.68142\n",
      "1680\n",
      "79.110916\n",
      "1690\n",
      "77.39485\n",
      "1700\n",
      "76.52173\n",
      "1710\n",
      "80.28284\n",
      "1720\n",
      "74.96569\n",
      "1730\n",
      "77.42087\n",
      "1740\n",
      "77.44726\n",
      "1750\n",
      "79.206436\n",
      "1760\n",
      "78.393364\n",
      "1770\n",
      "77.61902\n",
      "1780\n",
      "77.15277\n",
      "1790\n",
      "79.347916\n",
      "1800\n",
      "74.64724\n",
      "1810\n",
      "76.34944\n",
      "1820\n",
      "80.97913\n",
      "1830\n",
      "78.5931\n",
      "1840\n",
      "76.52576\n",
      "1850\n",
      "80.182655\n",
      "1860\n",
      "78.38856\n",
      "1870\n",
      "76.17142\n",
      "1880\n",
      "75.99375\n"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "num_iters = 10000\n",
    "# learning_rate = 0.001\n",
    "batch_size = 500\n",
    "for iteration in range(num_iters):\n",
    "    loss = 0\n",
    "    rnn.zero_grad()\n",
    "    for b in get_minibatch(batch_size):\n",
    "        sentence_chars = b[0]\n",
    "        target_next_chars = b[1]\n",
    "        h = Variable(torch.zeros(1,n_hidden))\n",
    "        len_sentence = sentence_chars.shape[0]\n",
    "        # I had some problems with very long sentences before, not sure why. Limiting it to 100 chars for now\n",
    "        # but it's something to check out later.\n",
    "        for i in range(min(len_sentence,100)):\n",
    "            input_char = Variable(sentence_chars[i,:,:])\n",
    "            loglet,_,h = rnn(input_char,h)\n",
    "            target_char = Variable(torch.LongTensor([target_next_chars[i]]))\n",
    "            loss += criterion(loglet,target_char)\n",
    "    #         lets.append(let)\n",
    "    loss /= batch_size\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "# below is if we wanted to use SGD\n",
    "#     for p in rnn.parameters():\n",
    "#         p.data.add_(-learning_rate, p.grad.data)\n",
    "    if iteration%(ceil(num_iters/1000))==0:\n",
    "        print(iteration)\n",
    "        print(loss.data.numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 191.1607\n",
       "[torch.FloatTensor of size (1,)]"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving trained net\n",
    "import pickle\n",
    "# pickle.dump(rnn.state_dict(), open(\"trained_simple_rnn2.pkl\",\"wb\"))\n",
    "rnn.load_state_dict(pickle.load(open(\"trained_simple_rnn.pkl\",\"rb\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code graveyard\n",
    "# targets[0]\n",
    "# let = Variable(d[0,:,:])\n",
    "# h = Variable(torch.zeros(1,n_hidden))\n",
    "# loss.backward()\n",
    "# let,h = rnn(let,h)\n",
    "# let,h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
